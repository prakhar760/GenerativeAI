{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO897TefA9g/1Z7dSIM6dIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakhar760/GenerativeAI/blob/main/Diffusers/Diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZs-1ZkanefE"
      },
      "outputs": [],
      "source": [
        "# install diffusers\n",
        "!pip install diffusers==0.11.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the google/ddpm-celebahq-256 model, built in collaboration by Google and U.C.Berkeley. It's a model following the Denoising Diffusion Probabilistic Models (DDPM) algorithm trained on a dataset of celebrities images. We can import the DDPMPipeline, which will allow you to do inference with a couple of lines of code.\n",
        "from diffusers import DDPMPipeline"
      ],
      "metadata": {
        "id": "qm2vSEhnntty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The from_pretrained() method allows downloading the model and its configuration from the Hugging Face Hub\n",
        "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "image_pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "JTrs6tlIqV8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate an image, we simply run the pipeline and don't even need to give it any input, it will generate a random initial noise sample and then iterate the diffusion process.\n",
        "images = image_pipe().images"
      ],
      "metadata": {
        "id": "7puD4qWxnyG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the generated image.\n",
        "images[0]"
      ],
      "metadata": {
        "id": "AmNFtbH_n5AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to view pipeline configurations.\n",
        "image_pipe"
      ],
      "metadata": {
        "id": "JdUfAPhqn5o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a simple unconditional image generation model of type UNet2DModel which was released with the DDPM Paper and for instance take a look at another checkpoint trained on church images: google/ddpm-church-256.\n",
        "from diffusers import UNet2DModel"
      ],
      "metadata": {
        "id": "V8VzDigSn_hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to view model configuration\n",
        "model.config"
      ],
      "metadata": {
        "id": "meodtO_4oGv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the config as an unpacked dict to the UNet2DModel class\n",
        "model_random = UNet2DModel(**model.config)"
      ],
      "metadata": {
        "id": "8enHx5XdoKd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the created model\n",
        "model_random.save_pretrained(\"my_model\")"
      ],
      "metadata": {
        "id": "iIkumejooN_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a random gaussian sample in the shape of an image (batch_size × in_channels × sample_size × sample_size). We have a batch axis because a model can receive multiple random noises, a channel axis because each one consists of multiple channels (such as red-green-blue), and finally sample_size corresponds to the height and width\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "noisy_sample = torch.randn(\n",
        "    1, model.config.in_channels, model.config.sample_size, model.config.sample_size\n",
        ")\n",
        "noisy_sample.shape"
      ],
      "metadata": {
        "id": "ndPZ1ZD6odym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    noisy_residual = model(sample=noisy_sample, timestep=2).sample"
      ],
      "metadata": {
        "id": "3rHdqVpWohT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The predicted noisy_residual has the exact same shape as the input and we use it to compute a slightly less noised image. Let's confirm the output shapes match\n",
        "noisy_residual.shape"
      ],
      "metadata": {
        "id": "qFAhxyAZoril"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load DDPMScheduler\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "scheduler = DDPMScheduler.from_config(repo_id)"
      ],
      "metadata": {
        "id": "K8muDSILowji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.config"
      ],
      "metadata": {
        "id": "dCp9uFwYo08w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Like the models, schedulers can be saved and loaded with save_config() and from_config().\n",
        "scheduler.save_config(\"my_scheduler\")\n",
        "new_scheduler = DDPMScheduler.from_config(\"my_scheduler\")"
      ],
      "metadata": {
        "id": "PKBqqhIpo3lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "less_noisy_sample = scheduler.step(\n",
        "    model_output=noisy_residual, timestep=2, sample=noisy_sample\n",
        ").prev_sample\n",
        "less_noisy_sample.shape"
      ],
      "metadata": {
        "id": "nrP8MLNSo6I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a display function that takes care of post-processing the denoised image, convert it to a PIL.Image and displays it.\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)"
      ],
      "metadata": {
        "id": "eegYe8Uso_1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move the input and model to the GPU to speed up the denoising process a bit.\n",
        "model.to(\"cuda\")\n",
        "noisy_sample = noisy_sample.to(\"cuda\")"
      ],
      "metadata": {
        "id": "yzLTACjopAvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the denoising loop. This loop prints out the (less and less) noisy samples along the way for better visualization in the denoising loop.\n",
        "import tqdm\n",
        "\n",
        "sample = noisy_sample\n",
        "\n",
        "for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
        "  # 1. predict noise residual\n",
        "  with torch.no_grad():\n",
        "      residual = model(sample, t).sample\n",
        "\n",
        "  # 2. compute less noisy image and set x_t -> x_t-1\n",
        "  sample = scheduler.step(residual, t, sample).prev_sample\n",
        "\n",
        "  # 3. optionally look at image\n",
        "  if (i + 1) % 50 == 0:\n",
        "      display_sample(sample, i + 1)"
      ],
      "metadata": {
        "id": "FkGReNwrpFuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load DDIMScheduler\n",
        "from diffusers import DDIMScheduler\n",
        "\n",
        "scheduler = DDIMScheduler.from_config(repo_id)"
      ],
      "metadata": {
        "id": "xCbxj4SmpGlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the DDIM inference steps to 50\n",
        "scheduler.set_timesteps(num_inference_steps=50)"
      ],
      "metadata": {
        "id": "leXyOStMpJ1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "sample = noisy_sample\n",
        "\n",
        "for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
        "  # 1. predict noise residual\n",
        "  with torch.no_grad():\n",
        "      residual = model(sample, t).sample\n",
        "\n",
        "  # 2. compute previous image and set x_t -> x_t-1\n",
        "  sample = scheduler.step(residual, t, sample).prev_sample\n",
        "\n",
        "  # 3. optionally look at image\n",
        "  if (i + 1) % 10 == 0:\n",
        "      display_sample(sample, i + 1)"
      ],
      "metadata": {
        "id": "ZD3skFLXpPZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uS8RNAcVpQHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}